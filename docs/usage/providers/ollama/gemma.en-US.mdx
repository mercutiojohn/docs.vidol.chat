---
title: Using the Google Gemma Model
description: Easily use the Google Gemma model for natural language processing tasks through the integration of LobeVidol and Ollama. Install Ollama, pull the Gemma model, select the Gemma model in the model panel, and start the conversation.
---

# Using the Google Gemma Model

<img
src="https://oss.vidol.chat/docs/2024/12/56d06a68955858385bd51dc590eb5156.png"
alt="Using Gemma in LobeVidol"
className="rounded-lg"
/>

[Gemma](https://blog.google/technology/developers/gemma-open-models/) is an open-source large language model (LLM) developed by Google, designed to provide a more general and flexible model for various natural language processing tasks. Now, with the integration of LobeVidol and [Ollama](https://ollama.com/), you can easily use Google Gemma in LobeVidol.

## Model Overview

Gemma is a series of open-source large language models launched by Google, available in 2B and 7B sizes. These models have the following features:

- Built on Gemini technology
- Supports multilingual processing capabilities
- Rigorously tested for safety
- Suitable for various natural language processing tasks

## Usage Guide

This document will guide you on how to use Google Gemma in LobeVidol:

### Step 1: Install Ollama Locally

First, you need to install Ollama. For the installation process, please refer to the [Ollama Usage Documentation](/zh/docs/usage/providers/ollama).

### Step 2: Pull the Google Gemma Model Locally with Ollama

After installing Ollama, you can install the Google Gemma model using the following commands:

```bash
# Install the 7B model
ollama pull gemma

# Or install the 2B model
ollama pull gemma:2b
```

<img
src="https://oss.vidol.chat/assets/a913a0827535d10e79a2f2c2593d6ac2.webp"
alt="Pulling the Gemma model with Ollama"
className="rounded-lg"
/>

### Step 3: Select the Gemma Model

On the session page, open the model panel and select the Gemma model.

<img
src="https://oss.vidol.chat/assets/78deb22856ec7b50ae7cc782c743598f.webp"
alt="Selecting the Gemma model in the model selection panel"
className="rounded-lg"
/>

<Note>
  If you do not see the Ollama provider in the model selection panel, please refer to [Integrating with Ollama](/zh/docs/self-hosting/examples/ollama) to learn how to enable the Ollama provider in LobeVidol.
</Note>

## Advanced Configuration

You can optimize the performance of Gemma by modifying the model parameters:

| Parameter Name  | Description               | Recommended Value |
| ---------------- | ------------------------- | ------------------ |
| temperature      | Controls the randomness of the output | 0.7                |
| top\_p           | Controls the diversity of the output  | 0.9                |
| max\_tokens      | Maximum length of a single response   | 4096               |

## Use Cases

The Gemma model is suitable for various scenarios:

- üí¨ Daily conversations and Q&A
- üìù Text generation and creation
- üîç Information extraction and summarization
- üéØ Domain-specific knowledge Q&A

## Frequently Asked Questions

<Accordion title="How much storage space does the Gemma model require?">
  - The Gemma 7B model requires approximately 4GB of storage space.
  - The Gemma 2B model requires approximately 1.5GB of storage space.
</Accordion>

<Accordion title="How can I improve the model's response speed?">
  - Using a newer GPU can significantly improve inference speed.
  - Appropriately reduce the max_tokens parameter.
  - Choose a smaller model version (e.g., 2B).
</Accordion>

## Related Resources

- [Gemma Official Documentation](https://blog.google/technology/developers/gemma-open-models/)
- [Ollama Official Website](https://ollama.com/)
- [LobeVidol Documentation Center](/zh/docs)

